---
title: "DM2_questions_2-3"
author: "AIGOIN Emilie"
date: "2025-11-22"
output: html_document
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)

```

# Introduction

L’Analyse Discriminante PLS exploratoire (ADPLS) est une méthode multivariée qui combine les principes de la régression PLS (Partial Least Squares) et de l’analyse discriminante.  
Son objectif est de construire des composantes linéaires des variables explicatives \( X \) maximisant la capacité de discrimination des groupes définis par la variable qualitative \( Y \).

Autrement dit, on cherche à extraire des axes factoriels \( f = X M u \) expliquant à la fois une grande part de la variance de \( X \) (inertie totale) et une forte part de la variance expliquée par les classes de \( Y \).  
Les notations utilisées sont celles du cours :

- \( X \in \mathbb{R}^{n \times p} \) : matrice des variables quantitatives centrées ;  
- \( Y \in \mathbb{R}^{n \times q} \) : matrice d’indicatrices des \( q \) classes (non centrée) ;  
- \( W = \mathrm{diag}(w_i) \) : matrice diagonale des poids individuels ;  
- \( M \) : métrique définie positive sur l’espace des variables ;  
- \( f = X M u \) : composante discriminante associée à la direction \( u \) ;  
- \( \Pi_Y = Y (Y' W Y)^{-1} Y' W \) : projecteur sur l’espace engendré par \( Y \) ;  
- \( \widehat{X} = \Pi_Y X \) : projection de \( X \) sur cet espace.

# Partie 2 — Programmation

Dans cette seconde partie, nous détaillons l’implémentation algorithmique de l’Analyse Discriminante PLS (ADPLS).

L’objectif est de reconstruire pas à pas les éléments de la méthode : extraction des composantes discriminantes, calcul des indicateurs associés et préparation des représentations graphiques permettant l’interprétation multivariée des données.

Nous adoptons une approche modulaire afin de distinguer clairement les opérations matricielles générales (produits pondérés, normalisations, métriques) des étapes spécifiques au calcul des composantes ADPLS.

## Définition des variables d’entrée et fonctions utilitaires

Les données d’entrée nécessaires à l’algorithme sont les suivantes :

- la matrice $X$ de dimension $n \times p$ : elle contient les variables explicatives numériques (non nécessairement centrées).
- la matrice $Y$ de dimension $n \times q$ : elle est formée des indicatrices correspondant aux $q$ modalités de la variable de groupe.
- le paramètre $H$ : il représente le nombre de composantes discriminantes à extraire.
- le vecteur de poids $w$, de longueur $n$ : il peut être laissé nul pour signifier des poids égaux.
- la matrice métrique $M$ de dimension $p \times p$ : elle est éventuellement identitaire.

L'option `center_scale` permet de centrer et réduire les variables de $X$ si nécessaire.

```{r setup_inputs, eval=FALSE, echo=TRUE}

# Définition des entrées (exemple à ne pas exécuter)

X <- as.matrix(X)                 # matrice n x p
Y <- as.matrix(Y)                 # matrice n x q
H <- 3                            # nombre de composantes
w <- rep(1 / nrow(X), nrow(X))    # poids égaux
M <- diag(ncol(X))                # matrice métrique identité
center_scale <- TRUE              # centrage-réduction activée

```

Afin d’assurer une formulation claire et homogène des opérations, nous définissons quelques fonctions utiles. Elles permettent notamment de gérer les produits scalaires pondérés, les normes induites par un vecteur de poids, ainsi que les transformations associées à la métrique $M$.

```{r setup_inputs_1, echo=TRUE}

# Matrice de pondération diagonale à partir d’un vecteur w

Wmat <- function(w) {
  if (is.null(w)) return(NULL)
  diag(as.numeric(w), nrow = length(w))
}

# Norme pondérée avec w vecteur poids ou W matrice diag

w_norm2 <- function(v, w) {
  if (is.null(w)) return(as.numeric(t(v) %*% v))
  as.numeric(t(v) %*% (w * v))
}

# Produit scalaire pondéré

w_prod <- function(a, b, w) {
  if (is.null(w)) return(as.numeric(t(a) %*% b))
  as.numeric(t(a) %*% (w * b))
}

# Racine symétrique de M (M supposée SPD ou semi-définie)

M_half <- function(M) {
  ev <- eigen(M, symmetric = TRUE)
  vals <- pmax(ev$values, 0)
  vecs <- ev$vectors
  vecs %*% diag(sqrt(vals), length(vals)) %*% t(vecs)
}

# Inverse de la racine symétrique de M (pseudo-inverse si valeurs nulles)

M_half_inv <- function(M) {
  ev <- eigen(M, symmetric = TRUE)
  vals <- ev$values
  vecs <- ev$vectors
  invvals <- ifelse(vals > 0, 1 / sqrt(vals), 0)
  vecs %*% diag(invvals, length(invvals)) %*% t(vecs)
}

```

## 1. Calcul des H premières composantes ADPLS

La première étape consiste à extraire successivement les $H$ composantes discriminantes. Pour chaque rang $h$, nous cherchons la composante $f_h = XMu_h$ qui maximise le critère $\Vert \widehat X M u \Vert^2_W$, où $\widehat X = \Pi_Y X$ désigne la projection de $X$ sur l'espace engendré par les indicatrices de $Y$.

D'après la partie théorique (question 2.b), pour $h = 1$, le vecteur $u_1$ est ke vecteur propre $M$-unitaire de la matrice $EM$ (où $E = \widehat X' W \widehat X$) associé à sa plus grande valeur propre. En pratique, nous diagonalisons la matrice symétrique $E^* = M^{1/2} E M^{1/2}$ pour obtenir $u_1^* = M^{1/2} u_1$, puis nous revenons à $u_1 = M^{-1/2}u_1^*$ en normalisant selon la contrainte $u'_1 M u_1 = 1$.

Pour $h > 1$, nous devons imposer l'orthogonalité $u'_h M u_h' = 0$ pour tout $h' < h$, ce qui revient à recherche $u_h$ dans l'orthogonal de $D = X'WF_{h-1}$ (où $F_{h-1} = [f_1, \dots, f_{h-1}]$). D'après la question 3.c, nous diagonalisons alors la matrice symétrique :
$$ M^{1/2} \Pi_{D^\perp} EM \Pi'_{D^\perp}M^{1/2} $$
où $Pi_{D^\perp} = I - D(D'MD)^{-1}D'M$ est le projecteur sur $D^\perp$ dans la métrique $M$.

L’algorithme ci-dessous réalise ces étapes successives et renvoie les vecteurs $u_h$, les composantes discriminantes $F = (f_h)$, les composantes normalsiées $\tilde F$, ainsi que les quantités intermédiaires $\widehat X$, $E$ et le projecteur $\Pi_Y$.

```{r adpls_part1, echo=TRUE}

adpls_part1 <- function(X, Y, H = 2, w = NULL, M = NULL, center_scale = TRUE) {
  
# Conversion explicite en matrices et extraction des dimensions
  X <- as.matrix(X)
  Y <- as.matrix(Y)
  n <- nrow(X)
  p <- ncol(X)
  
# Vérification : X et Y doivent décrire les mêmes individus
  if (nrow(Y) != n) stop(" X et Y doivent avoir le même nombre de lignes (individus)")

# Construction du vecteur de poids w 
  if (is.null(w)) w <- rep(1 / n, n)
  w <- as.numeric(w)
  if (length(w) != n) stop("Longueur de w différente de n (nrow(X))")
  W <- diag(w)
  
# Centrage-réduction éventuel de X
  if (center_scale) X <- scale(X, center = TRUE, scale = TRUE)
  
# Matrice métrique M (M = I par défaut)
  if (is.null(M)) M <- diag(1, p)
  if (!all(dim(M) == c(p, p))) stop("M doit être p x p")
  
# Calcul de racines de M
  sqrtM <- M_half(M)
  inv_sqrtM <- M_half_inv(M)
  
# Projecteur Pi_Y
  YYw <- t(Y) %*% (W %*% Y)
  inv_YYw <- solve(YYw)
  PiY <- Y %*% inv_YYw %*% t(Y) %*% W
  
# Projection de X sur l'espace de Y
  Xhat <- PiY %*% X
  
# Matrice E (pour la diagonalisation)
  E <- t(Xhat) %*% (W %*% Xhat)
  
# Réservation des objets de sortie
  if (is.null(H) || !is.numeric(H) || H < 1) stop("H doit être un entier positif")
  H <- as.integer(H)
  U_list <- vector("list", H) 
  F <- matrix(0, n, H)
  Ftilde <- matrix(0, n, H)
  
# Boucle sur h = 1...H
  for (h in seq_len(H)) {
    if (h == 1) {
      
# Première composante : diagonalisation de E* = M^{1/2} E M^{1/2}
      Estar <- sqrtM %*% E %*% sqrtM
      ev <- eigen(Estar, symmetric = TRUE)
      
# Vecteur propre principal en métrique transformée
      ustar <- ev$vectors[, 1]
      
# Retour dans l'espace u : u = M^{-1/2} u*
      u <- inv_sqrtM %*% ustar
      
# Normalisation selon la contrainte u'Mu = 1
      unorm <- as.numeric(t(u) %*% (M %*% u))
      if (unorm <= 0) stop("Norme u'Mu non strictement positive")
      u <- u / sqrt(unorm)
      
    } else {
      
# Construction de D' = F_{h-1}' W X
      Fprev <- F[, 1:(h - 1), drop = FALSE]
      Dprime <- t(Fprev) %*% (W %*% X)
      D <- t(Dprime)
      
# Construction de Pi_{D^\perp} = I - D(D'MD)^{-1}D'M
      inner <- t(D) %*% (M %*% D)
      inv_inner <- solve(inner)
      PiDperp <- diag(1, p) - D %*% inv_inner %*% t(D) %*% M
      
# Matrice à diagonaliser : M^{1/2} Pi_{D^\perp} E M Pi_{D^\perp}' M^{1/2}
      Estar_proj <- sqrtM %*% PiDperp %*% E %*% t(PiDperp) %*% sqrtM
      ev <- eigen(Estar_proj, symmetric = TRUE)
      
# Premier vecteur propre dans l’espace réduit
      ustar <- ev$vectors[, 1]
      u <- inv_sqrtM %*% ustar
      
# Normalisation selon la contrainte u'Mu = 1
      unorm <- as.numeric(t(u) %*% (M %*% u))
      if (unorm <= 0) stop(paste("Norme u'Mu non positive (h=", h, ")", sep = ""))
      u <- u / sqrt(unorm)
    }
    
# Calcul du score f_h = X M u
    f_h <- X %*% (M %*% u)
    
# Normalisation pondérée : f_tilde_h = f_h / ||f_h||_W
    fnorm2 <- as.numeric(t(f_h) %*% (W %*% f_h))
    if (fnorm2 <= 0) stop(paste("Norme de f_h <= 0 (h=", h, ")", sep = ""))
    ftilde_h <- f_h / sqrt(fnorm2)
    
# Stockage des résultats
    U_list[[h]] <- as.numeric(u)
    F[, h] <- as.numeric(f_h)
    Ftilde[, h] <- as.numeric(ftilde_h)
  }
  
# Conversion de la liste de vecteurs en matrice p x H
  U_mat <- do.call(cbind, U_list)
  colnames(U_mat) <- paste0("u", seq_len(ncol(U_mat)))
  colnames(F) <- paste0("f", seq_len(ncol(F)))
  colnames(Ftilde) <- paste0("f_tilde", seq_len(ncol(Ftilde)))
  
  return(list(U = U_mat, F = F, Ftilde = Ftilde, Xhat = Xhat, 
              E = E, PiY = PiY, M = M, w = w))
}
```

## 2. Calcul des indicateurs $S(f_h)$ et $R^2 (f_h, Y)$

Dans cette étape, nous évaluons la qualité des composantes discriminantes obtenues à partir de l’ADPLS. Deux indicateurs complémentaires sont utilisés :

$$ S(f_h) = \frac{\| f_h \|_W^2}{\operatorname{tr}(X' W X)}, \quad 
R^2(f_h, Y) = \frac{\| \hat{f}_h \|_W^2}{\| f_h \|_W^2} $$

où $\hat{f}_h = \Pi_Y f_h$ désigne la projection de $f_h$ sur l'espace engendré par $Y$.

L’indicateur $S(f_h)$ mesure la proportion d’inertie totale de $X$ capturée par la composante $f_h$.
Une valeur élevée signale que l’axe contribue significativement à la structuration globale du nuage d’individus.

L’indicateur $R^2(f_h,Y)$ quantifie la part de variance de $f_h$, expliquée par les classes. Un $R^2$ proche de 1 indique que la composante est fortement alignée avec la structure induite par la variable de groupe et possède une capacité discriminante élevée.

L’implémentation suivante exploite les résultats de la fonction `adpls_part1` et procède, pour chaque composante, au calcul des normes pondérées nécessaires à l’obtention des deux indicateurs.

```{r adpls_part2, echo=TRUE}

adpls_part2 <- function(X, Y, H = 2, w = NULL, M = NULL, center_scale = TRUE) {
  
# Réutilise la fonction adpls_part1
  part1 <- adpls_part1(X, Y, H, w, M, center_scale)
  
# Préparation des objets utiles
  X <- as.matrix(X)
  if (center_scale) X <- scale(X,             # centrer-réduire X si nécessaire
                               center = TRUE, 
                               scale = TRUE)   
  n <- nrow(X)
  W <- diag(part1$w)
  M <- part1$M
  F <- part1$F
  U <- part1$U
  Xhat <- part1$Xhat
  E <- part1$E
  
# Calcul de l'inertie totale sur X transformé
  total_inertia <- sum(diag(t(X) %*% (W %*% X)))
  
# Réservation des vecteurs résultats
  S_vec <- numeric(H)
  R2_vec <- numeric(H)
  
# Boucle sur h
  for (h in seq_len(H)) {
    
# Extraction de f_h
    f_h <- F[, h, drop = FALSE]
    
# Norme pondérée
    fnorm2 <- as.numeric(t(f_h) %*% (W %*% f_h))
    
# Calcul de S(f_h)
    S_vec[h] <- fnorm2 / total_inertia
    
# Projection PiY*f_h
    fhat_h <- part1$PiY %*% f_h
    
# Norme pondérée de la projection
    top_val <- as.numeric(t(fhat_h) %*% (W %*% fhat_h))
    
# Calcul du coefficient de détermination R²
    R2_vec[h] <- top_val / fnorm2
  }
  
# Ajout des indicateurs au résultat final
  part1$S <- S_vec
  part1$R2 <- R2_vec
  
  return(part1)
}

```

Le vecteur `S_vec` regroupe les proportions d’inertie globale expliquées par chacune des composantes discriminantes, tandis que le vecteur `R2_vec` fournit une mesure synthétique de la part de variance des composantes imputable aux classes.

Ces deux indicateurs constituent la base de l’interprétation exploratoire de l’ADPLS :
les composantes associées à un $S(f_h)$ élevé et un $R^2(f_h, Y)$ proche de 1 sont celles qui contribuent le plus efficacement à la discrimination entre groupes.

## 3. Calcul des coordonnées des centres de gravité des $q$ classes sur les $H$ axes discriminants

Cette étape consiste à déterminer, pour chacune des $q$ classes, les coordonnées de son centre de gravité dans l’espace défini par les $H$ premières composantes discriminantes.

Rappelons que $F = [f_1, f_2, \ldots, f_H]$ désigne la matrice des composantes discriminantes de dimension $(n \times H)$, et que $Y$ est la matrice indicatrice $(n \times q)$ telle que $Y_{ik} = 1$ si l'individu $i$ appartient à la classe $k$, et $0$ sinon.

La coordonnée du centre de gravité de la classe $k$ sur l’axe $h$ correspond à la moyenne pondérée des scores individuels sur cet axe :

$$ g_{kh} = \frac{ \sum_{i=1}^{n} w_i \, Y_{ik} \, f_{ih} }{ \sum_{i=1}^{n} w_i \, Y_{ik} } $$

Sous forme matricielle, la matrice $G \in \mathbb R^{q \times H}$ des coordonnées de tous les cetres de gravité s'écrit :
$$ G = (Y' W Y)^{-1} Y' W F $$

où chaque ligne de $G$ correspond au barycentre d'une classe dans l'espace factoriel, et chaque colonne correspond à un axe discriminant.

L’implémentation est donnée ci-dessous.

```{r adpls_part3, echo=TRUE}

adpls_part3 <- function(X, Y, H = 2, w = NULL, M = NULL, center_scale = TRUE) {

# Réutilise la fonction adpls_part2
  part2 <- adpls_part2(X, Y, H, w, M, center_scale)
  
# Préparation des objets utiles
  W <- diag(part2$w)
  F <- part2$F
  Y <- as.matrix(Y)
  
# Calcul des centres de gravité : G = (Y'WY)^{-1} Y'WF
  YtWY <- t(Y) %*% W %*% Y
  YtWF <- t(Y) %*% W %*% F
  G <- solve(YtWY, YtWF)
  
  rownames(G) <- colnames(Y)
  colnames(G) <- paste0("Axe_", seq_len(H))
  
# Ajout des résultats
  part2$centers <- G
  
  return(part2)
}

```

La matrice $G$ contient donc les coordonnées pondérées des centres de gravité des classes dans le sous-espace discriminant. Chaque ligne de $G$ représente la position barycentrique d’une classe sur les $H$ axes principaux.

Ces coordonnées constituent une étape essentielle pour l’interprétation géométrique : elles permettent de visualiser la disposition relative des groupes dans le plan factoriel $(h, m)$, ce qui sera exploité lors de la représentation graphique.

## 4. Calcul des coordonnées des variables de X

Cette étape consiste à déterminer la position des variables initiales $X_j$ dans l’espace engendré par les $H$ omposantes discriminantes obtenues par l’ADPLS. L’étude de ces coordonnées permet d’évaluer la contribution de chaque variable à la construction des axes discriminants et, par conséquent, son rôle dans la discrimination entre les groupes.

Pour une variable centrée-réduite  $X_j$ pet une composante discriminante $f_h$, la corrélation pondérée est définie par :

\[
r_{jh} =  \frac{ \langle X_j , f_h \rangle_W }
               { \|X_j\|_W \, \|f_h\|_W }
\]

où :

- $X_j$ : $j$-ième variable centrée-réduite de $X$,
- $f_h$ : $h$-ième composante discriminante,
- $\langle \cdot , \cdot \rangle_W$ : le produit scalaire pondéré par la matrice $W$,
- $\|\cdot\|_W$ :  la norme associée à ce produit scalaire.

Sous forme matricielle, la matrice des corrélations entre les variables et les axes discriminants s’écrit :

\[
C = \text{corr}(X, F) = D_X^{-1} \, (X' W F) \, D_F^{-1}
\]

avec :

- $D_X$ la matrice diagonale des normes pondérées des variables de $X_j$,
- $D_F$ la matrice diagonale des normes pondérées des composantes $f_h$.

La matrice $C$ (de dimension $p \times H$) regroupe les coordonnées factorielles des variables sur les différents axes.

```{r adpls_part4, echo=TRUE}
adpls_part4 <- function(X, Y, H = 2, w = NULL, M = NULL, center_scale = TRUE) {

# réutilise la fonction adpls_part3
  part3 <- adpls_part3(X, Y, H, w, M, center_scale)
  
# préparation des objets utiles
  W <- diag(part3$w)
  F <- part3$F
  
# X supposé centré-réduit
  X <- scale(X, center = TRUE, scale = TRUE)
  n <- nrow(X)
  p <- ncol(X)
  
# matrice des corrélations
  C <- matrix(0, nrow = p, ncol = H)
  
# corrélations pondérées entre X_j et f_h
  for (j in 1:p) {
    xj <- X[, j]
    norm_xj <- sqrt(as.numeric(t(xj) %*% (W %*% xj)))
    
    for (h in 1:H) {
      fh <- F[, h]
      norm_fh <- sqrt(as.numeric(t(fh) %*% (W %*% fh)))
      if (norm_xj == 0 || norm_fh == 0) {
        C[j, h] <- 0
      } else {
        C[j, h] <- as.numeric(t(xj) %*% (W %*% fh)) / (norm_xj * norm_fh)
      }
    }
  }
  
  rownames(C) <- colnames(X)
  colnames(C) <- paste0("Axe_", seq_len(H))
  
# résultats
  part3$var_coords <- C
  
  return(part3)
}
```

Les coefficients de la matrice $C$ représentent les corrélations pondérées entre les variables initiales et les axes discriminants issus de l’ADPLS. Une valeur absolue élevée (proche de $1$) indique une forte contribution de la variable à la construction de l’axe considéré, tandis qu’une valeur faible (proche de $0$) traduit une influence limitée, voire nulle.

Ainsi, l’analyse de la matrice $C$ constitue une étape fondamentale pour relier les composantes discriminantes aux variables d’origine. Elle permet d’interpréter la signification géométrique des axes obtenus, en identifiant les variables qui structurent le plus efficacement la séparation entre les groupes.

### Fonction générale — intégration du processus ADPLS

Afin de rassembler l’ensemble des étapes de calcul présentées précédemment (questions 1 à 4), nous avons implémenté une fonction générale `adpls_full()`.

Cette fonction constitue une synthèse opérationnelle de l’approche ADPLS et permet d’obtenir, à partir des matrices d’entrée $X$ et $Y$, l’ensemble des résultats requis pour l’analyse discriminante partielle par moindres carrés.

```{r adpls_full}
adpls_full <- function(X, Y, H=2, w=NULL, M=NULL, center_scale=TRUE){
  
# fonctions utilitaires internes
Wmat <- function(w) {
  if (is.null(w)) return(NULL)
  diag(as.numeric(w), nrow = length(w))
}

w_norm2 <- function(v, w) {
  if (is.null(w)) return(sum(v^2))
  as.numeric(t(v) %*% (w * v))
}

w_prod <- function(a, b, w) {
  if (is.null(w)) return(as.numeric(t(a) %*% b))
  as.numeric(t(a) %*% (w * b))
}

M_half <- function(M) {
  ev <- eigen(M, symmetric = TRUE)
  vals <- pmax(ev$values, 0)
  vecs <- ev$vectors
  vecs %*% diag(sqrt(vals), length(vals)) %*% t(vecs)
}

M_half_inv <- function(M) {
  ev <- eigen(M, symmetric = TRUE)
  vals <- ev$values
  vecs <- ev$vectors
  invvals <- ifelse(vals > 0, 1 / sqrt(vals), 0)
  vecs %*% diag(invvals, length(invvals)) %*% t(vecs)
}
 
 if (is.null(w)) w <- rep(1 / nrow(X), nrow(X))
  
# étapes de calcul
  part1 <- adpls_part1(X = X, Y = Y, H = H, w = w, M = M, center_scale = center_scale)
  part2 <- adpls_part2(X = X, Y = Y, H = H, w = w, M = M, center_scale = center_scale)
  part3 <- adpls_part3(X = X, Y = Y, H = H, w = w, M = M, center_scale = center_scale)
  part4 <- adpls_part4(X = X, Y = Y, H = H, w = w, M = M, center_scale = center_scale)

 # résultats 
  return(list(F=part1$F, 
              Ftilde=part1$Ftilde,
              S=part2$S, 
              R2=part2$R2,
              centers=part3$centers,
              var_coords=part4$var_coords))
}
```

## 5. Représentation graphique des individus, des centres de gravité et des variables

Dans cette dernière étape, nous devons visualiser les résultats obtenus dans les étapes précédentes.  

L’objectif comporte trois volets :

1. Afficher les individus dans le plan factoriel $(h, m)$ choisi par l'utilisateur.
2. Afficher les centres de gravité des $q$ classes sur ce même plan.
3. Afficher les variables de $X$ projetées sur le plan choisi et tracer le cercle unité.

Pour cela, nous utilisons la fonction `plot_adpls()` qui génère la figure dans le plan $(h, m)$.

Cette fonction affiche :

- les individus colorés selon leur classe,
- les centres de gravité des $q$ classes,
- les variables de $X$ projetées sur le plan choisi,
- le cercle unité, pour faciliter l'interprétation des corrélations.

```{r plot_adpls, echo=TRUE }
library(RColorBrewer)

plot_adpls <- function(out, Y = NULL, Y_labels = NULL, h = 1, m = 2,
                       show_variables = TRUE, circle = TRUE,
                       main = NULL, cex_ind = 0.7, cex_cent = 1.0,
                       xlim = NULL, ylim = NULL) {

  Ftilde <- out$Ftilde
  centers <- out$centers
  var_coords <- out$var_coords
  n <- nrow(Ftilde)
  
# identification des étiquettes de classes
  if (is.null(Y_labels) & !is.null(Y)) {
    lab <- apply(Y, 1, function(r) which(r == 1))
    Y_labels <- factor(lab)
  } else if (is.null(Y_labels)) {
    stop("Y_labels ou Y doivent être fournis!")
  }
  
# individus
  xind <- Ftilde[, h]
  yind <- Ftilde[, m]
  classes <- as.factor(Y_labels)
  cols <- rainbow(length(levels(classes)))
  
# palette de couleurs
  n_classes <- length(levels(classes))
  cols <- brewer.pal(min(max(n_classes, 3), 8), "Set2")
  if (n_classes > 8) cols <- rep(cols, length.out = n_classes)
  
# tracé des individus
  plot(xind, yind,
       col = adjustcolor(cols[as.numeric(classes)], alpha.f = 0.7),
       pch = 19,
       cex = cex_ind,
       xlab = paste0("Axe ", h),
       ylab = paste0("Axe ", m),
       main = main,
       bg = "white",
       xlim = xlim,
       ylim = ylim)
  
legend("topright",
       legend = levels(classes),
       col = cols,
       pch = 19,
       cex = 1,
       title = "Classes",
       bg = "white",
       box.lwd = 0.5, 
       box.col = "black",
       text.col = "black")
  
# centres de gravité
  points(centers[, h], centers[, m], col = "black", pch = 8, cex = cex_cent)
  text(centers[, h], centers[, m], labels = rownames(centers), pos = 3)
  
# variables dans le plan
  if (show_variables) {
    arrows(rep(0, nrow(var_coords)), rep(0, nrow(var_coords)),
           var_coords[, h], var_coords[, m],
           length = 0.07, col = "grey40", lwd = 1)
    
    text(var_coords[, h], var_coords[, m],
         labels = rownames(var_coords),
         cex = 0.7, col = "grey20")
    
# cercle unité pour corrélations
    if (circle) {
      angles <- seq(0, 2*pi, length.out = 200)
      lines(cos(angles), sin(angles), lty = 2, col = "black")
    }
  }
}

# opérateur utilitaire : renvoie a si non nul, sinon b
`%||%` <- function(a, b) if (!is.null(a)) a else b
```

### Conclusion 

Dans cette deuxième partie, nous avons développé l’ensemble des étapes nécessaires à la mise en œuvre de l’ADPLS. Les fonctions programmées permettent d’obtenir les composantes discriminantes, d’évaluer leur qualité, de calculer les centres de gravité des classes et de mesurer la contribution des variables. La fonction graphique rassemble ensuite ces résultats dans une représentation claire sur les plans factoriels, facilitant ainsi l’interprétation. L’ensemble constitue un outil fonctionnel et accessible, qui automatise l’application de la méthode ADPLS et rend l’analyse plus efficace.

# Partie 3 — Application: types forestiers du bassin du Congo

## Chargement des données et ADPLS exploratoire

Tout d’abord, nous chargeons le fichier genus et réalisons une analyse ADPLS exploratoire de la variable $Y = forest$ sur les 27 variables de composition arborée $X = [gen1, …, gen27]$. Avant l’analyse, toutes les variables $X$ sont centrées et réduites.

```{r q1_dataset}
data <- read.table("Datagenus.csv", header = TRUE)
X <- scale(as.matrix(data[, paste0("gen", 1:27)]), center = TRUE, scale = TRUE)
grp <- as.factor(data$forest)
Y <- model.matrix(~ grp - 1)
```

Une attention particulière est portée au choix de la métrique $M$. Nous allons comparer deux options :

- $M = I$ — matrice identité, métrique standard sans pondération.
- $M = \text{diag}\left(\frac{1}{\text{var}(X_j)}\right)$ — inverse des variances des variables, pour tenir compte des différences d’échelle.

En comparant les résultats obtenus avec ces différentes métriques, nous pourrons choisir celle qui met le mieux en évidence la structure discriminante entre les types forestiers.

### Metrique 

$M = I$

```{r q1_m1}
M1 <- diag(1, ncol(X))

#adls
out1 <- adpls_full(X, Y, H = 3, w = rep(1 / nrow(X), nrow(X)), M = M1, center_scale = FALSE)

data.frame(
  Axe = 1:3,
  S = round(out1$S, 3),
  R2 = round(out1$R2, 3),
  Discriminant = round(out1$S * out1$R2, 3)
)
```

Le tableau des indicateurs montre que la première composante (axe 1) concentre la part la plus importante de l’inertie totale $(S_1 = 18{,}4\%)$ et qu’elle est la plus corrélée avec la variable de groupe $(R^2_1 = 0{,}20)$. Son pouvoir discriminant $( 0{,}037)$ est donc le plus élevé. Elle représente l’axe principal de séparation entre les types de forêts. La deuxième composante $(0{,}021)$ contribue de manière secondaire, tandis que la troisième $(0{,}005)$ joue un rôle marginal.

### Metrique 

$M = \text{diag}\left(\frac{1}{\text{var}(X_j)}\right)$

```{r q1_m2}
X2 <- as.matrix(data[, paste0("gen", 1:27)])
grp <- as.factor(data$forest)
Y <- model.matrix(~ grp- 1)

variances2 <- apply(X2, 2, var)
M2 <- diag(1 / variances2)

# standartisation
X_scaled2 <- scale(X, center = TRUE, scale = TRUE)

# adpls
out_M2 <- adpls_full(X_scaled2, Y, H = 3, M = M2)

# S, R2, Discriminant
data.frame(
  Axe = 1:3,
  S = round(out_M2$S, 3),
  R2 = round(out_M2$R2, 3),
  Discriminant = round(out_M2$S * out_M2$R2, 3)
)
```


## Interpretation des resultats

```{r q2 input}
data <- read.table("Datagenus.csv", header = TRUE)

X <- scale(as.matrix(data[, paste0("gen", 1:27)]), center = TRUE, scale = TRUE)
grp <- as.factor(data$forest)
Y <- model.matrix(~ grp - 1)

#choosing M
M <- diag(1, ncol(X))

#adls
out <- adpls_full(X, Y, H = 3, w = rep(1 / nrow(X), nrow(X)), M = M, center_scale = FALSE)

data.frame(
  Axe = 1:3,
  S = round(out$S, 3),
  R2 = round(out$R2, 3),
  Discriminant = round(out$S * out$R2, 3)
)
```


```{r q2_1-2}
plot12 <- plot_adpls(out, Y = Y, Y_labels = grp, h = 1, m = 2, show_variables = TRUE, main = "Plan 1-2")
```
Le graphique de projection sur le plan formé par les axes 1 et 2 illustre cette structure. Chaque point représente une forêt colorée selon sa classe, et les croix indiquent les centres de gravité des groupes. On observe une séparation nette entre les classes 4 et 7, qui apparaissent bien distinctes dans l’espace factoriel. Cela suggère que ces deux types de forêts possèdent des compositions en genres d’arbres clairement différenciées.

En revanche, les classes 5 et 6 sont proches l’une de l’autre : leurs centres de gravité sont situés dans une même zone du plan, ce qui traduit une similarité partielle dans leur composition floristique ou un recouvrement des caractéristiques discriminantes identifiées par les deux premières composantes. Cette proximité peut indiquer soit une continuité écologique entre ces deux types de forêts, soit une difficulté du modèle à les séparer de manière nette.

```{r bigplot1, fig.width=10, fig.height=10}
plot_adpls(
  out,
  Y = Y,
  Y_labels = grp,
  h = 1,
  m = 2,
  show_variables = TRUE,
  main = "Plan 1-2",
  xlim = c(-2, 2),
  ylim = c(-2, 2)
)
```



```{r q2_1-3}
plot13 <- plot_adpls(out, Y = Y, Y_labels = grp, h = 1, m = 3, show_variables = TRUE, main = "Plan 1-3")
```

```{r bigplot2, fig.width=10, fig.height=10}
plot_adpls(
  out,
  Y = Y,
  Y_labels = grp,
  h = 1,
  m = 3,
  show_variables = TRUE,
  main = "Plan 1-3",
  xlim = c(-2, 2),
  ylim = c(-2, 2)
)
```

```{r q2_2-3}
plot23 <- plot_adpls(out, Y = Y, Y_labels = grp, h = 2, m = 3, show_variables = TRUE, main = "Plan 2-3")
```

```{r bigplot3, fig.width=10, fig.height=10}
plot_adpls(
  out,
  Y = Y,
  Y_labels = grp,
  h = 2,
  m = 3,
  show_variables = TRUE,
  main = "Plan 2-3",
  xlim = c(-1, 1),
  ylim = c(-1, 1)
)
```
